{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.empty(3,2,3)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4426, 0.7409, 0.8603, 0.3879, 0.2431, 0.3335, 0.6146, 0.9796, 0.7555,\n",
      "        0.0615, 0.3999, 0.7253, 0.7734, 0.9195, 0.5578, 0.4578, 0.8037, 0.8701,\n",
      "        0.7046, 0.3209, 0.6055, 0.2285, 0.4833, 0.4731, 0.6716, 0.9812, 0.4381,\n",
      "        0.4777, 0.4990, 0.5341, 0.6605, 0.3511, 0.0128, 0.2608, 0.1229, 0.5036,\n",
      "        0.0175, 0.6067, 0.4872, 0.4903, 0.8915, 0.0916, 0.3428, 0.9127, 0.7646,\n",
      "        0.7178, 0.4001, 0.3830, 0.7651, 0.7408, 0.2625, 0.8800, 0.0028, 0.1596,\n",
      "        0.9785, 0.7382, 0.4191, 0.2321, 0.5644, 0.8294, 0.1983, 0.1974, 0.5303,\n",
      "        0.1874, 0.6179, 0.0193, 0.8084, 0.8224, 0.6680, 0.2490, 0.1010, 0.7278,\n",
      "        0.8242, 0.3807, 0.6227, 0.3443, 0.2071, 0.2190, 0.0151, 0.9818, 0.5490,\n",
      "        0.0306, 0.8704, 0.1221, 0.2002, 0.9497, 0.8802, 0.8014, 0.4796, 0.8999,\n",
      "        0.9947, 0.1212, 0.2014, 0.3392, 0.7356, 0.5059, 0.6651, 0.5821, 0.7488,\n",
      "        0.3811])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(100)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9146, 0.1632],\n",
      "        [0.7327, 0.5587]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = torch.add(x,y)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1415, 0.8358, 0.7322],\n",
      "        [0.3675, 0.7013, 0.9906],\n",
      "        [0.4377, 0.5748, 0.0595],\n",
      "        [0.9037, 0.7778, 0.7872],\n",
      "        [0.4913, 0.4702, 0.9494]])\n",
      "tensor([0.1415, 0.3675, 0.4377, 0.9037, 0.4913])\n",
      "0.701259970664978\n"
     ]
    }
   ],
   "source": [
    "# elements wise\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:,0])\n",
    "print(x[1,1].item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1329, 0.9868, 0.1906, 0.2309],\n",
      "        [0.5015, 0.5322, 0.7401, 0.2189],\n",
      "        [0.0503, 0.2390, 0.2259, 0.9718],\n",
      "        [0.0233, 0.7201, 0.7528, 0.8210]])\n",
      "tensor([[0.1329, 0.9868, 0.1906, 0.2309, 0.5015, 0.5322, 0.7401, 0.2189],\n",
      "        [0.0503, 0.2390, 0.2259, 0.9718, 0.0233, 0.7201, 0.7528, 0.8210]])\n"
     ]
    }
   ],
   "source": [
    "# reshape the tensor\n",
    "x= torch.rand(4,4)\n",
    "print(x)\n",
    "y=x.view(-1,8)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "You can see that torch tensor and numpy are share value each other.\n",
      "\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# convert torch to numpy\n",
    "import numpy as np\n",
    "a  = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n",
    "\n",
    "# remember that the values will share ehch other.\n",
    "a.add_(1)\n",
    "print(\"\\nYou can see that torch tensor and numpy are share value each other.\\n\")\n",
    "print(a)\n",
    "print(b)\n",
    "# If you want the values independent between numpy and torch.\n",
    "# type the following code\n",
    "b = a.numpy().copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert numpy to torch\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# create tensor and put it into the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5, device = device)\n",
    "    y - torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x+y\n",
    "    z = z.to(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7145, 0.4294, 1.2373], requires_grad=True)\n",
      "tensor([2.7145, 2.4294, 3.2373], grad_fn=<AddBackward0>)\n",
      "tensor([14.7374, 11.8040, 20.9599], grad_fn=<MulBackward0>)\n",
      "tensor([1.0858, 0.9718, 0.0129])\n"
     ]
    }
   ],
   "source": [
    "# Gradian calculation\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "# z = z.mean()\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([0.1,0.1,0.001], dtype=torch.float32)\n",
    "z.backward(v) # dz/dx\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8580, -0.9005, -0.5781], requires_grad=True)\n",
      "tensor([1.1420, 1.0995, 1.4219])\n"
     ]
    }
   ],
   "source": [
    "# preventing gradient history\n",
    "x=torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# x.requires_grad_(False)\n",
    "# y =  x.detach()\n",
    "# with torch.no_grad():\n",
    "#     y=x+2\n",
    "#     print(y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "import torch\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    print(model_output)\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()\n",
    "    print(weights)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n",
      "tensor(1., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# backpropagation\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "y_hat = x * w\n",
    "loss = (y_hat -y) ** 2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(w)\n",
    "\n",
    "# update weights\n",
    "# next forward and backward pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5): 0.0\n",
      "epoch 1: w= 0.300, loss=30.000\n",
      "epoch 3: w= 0.772, loss=15.660\n",
      "epoch 5: w= 1.113, loss=8.175\n",
      "epoch 7: w= 1.359, loss=4.267\n",
      "epoch 9: w= 1.537, loss=2.228\n",
      "epoch 11: w= 1.665, loss=1.163\n",
      "epoch 13: w= 1.758, loss=0.607\n",
      "epoch 15: w= 1.825, loss=0.317\n",
      "epoch 17: w= 1.874, loss=0.165\n",
      "epoch 19: w= 1.909, loss=0.086\n",
      "epoch 21: w= 1.934, loss=0.045\n",
      "epoch 23: w= 1.952, loss=0.024\n",
      "epoch 25: w= 1.966, loss=0.012\n",
      "epoch 27: w= 1.975, loss=0.006\n",
      "epoch 29: w= 1.982, loss=0.003\n",
      "epoch 31: w= 1.987, loss=0.002\n",
      "epoch 33: w= 1.991, loss=0.001\n",
      "epoch 35: w= 1.993, loss=0.000\n",
      "epoch 37: w= 1.995, loss=0.000\n",
      "epoch 39: w= 1.996, loss=0.000\n",
      "epoch 41: w= 1.997, loss=0.000\n",
      "epoch 43: w= 1.998, loss=0.000\n",
      "epoch 45: w= 1.999, loss=0.000\n",
      "epoch 47: w= 1.999, loss=0.000\n",
      "epoch 49: w= 1.999, loss=0.000\n",
      "epoch 51: w= 1.999, loss=0.000\n",
      "epoch 53: w= 2.000, loss=0.000\n",
      "epoch 55: w= 2.000, loss=0.000\n",
      "epoch 57: w= 2.000, loss=0.000\n",
      "epoch 59: w= 2.000, loss=0.000\n",
      "epoch 61: w= 2.000, loss=0.000\n",
      "epoch 63: w= 2.000, loss=0.000\n",
      "epoch 65: w= 2.000, loss=0.000\n",
      "epoch 67: w= 2.000, loss=0.000\n",
      "epoch 69: w= 2.000, loss=0.000\n",
      "epoch 71: w= 2.000, loss=0.000\n",
      "epoch 73: w= 2.000, loss=0.000\n",
      "epoch 75: w= 2.000, loss=0.000\n",
      "epoch 77: w= 2.000, loss=0.000\n",
      "epoch 79: w= 2.000, loss=0.000\n",
      "epoch 81: w= 2.000, loss=0.000\n",
      "epoch 83: w= 2.000, loss=0.000\n",
      "epoch 85: w= 2.000, loss=0.000\n",
      "epoch 87: w= 2.000, loss=0.000\n",
      "epoch 89: w= 2.000, loss=0.000\n",
      "epoch 91: w= 2.000, loss=0.000\n",
      "epoch 93: w= 2.000, loss=0.000\n",
      "epoch 95: w= 2.000, loss=0.000\n",
      "epoch 97: w= 2.000, loss=0.000\n",
      "epoch 99: w= 2.000, loss=0.000\n",
      " prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# get understand gradian with troch\n",
    "import torch\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True,)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss function with MSE\n",
    "def loss(y, y_predict):\n",
    "    return ((y_predict-y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5): {forward(5)}')\n",
    "\n",
    "# Training\n",
    "lr = 0.01\n",
    "n_iter = 100\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    # forward\n",
    "    y_predict = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_predict)\n",
    "    l.backward() # dl/dw\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w= {w:.3f}, loss={l:.3f}')\n",
    "\n",
    "print(f' prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5): -0.942479133605957\n",
      "epoch 1: w= -0.298, loss=32.983\n",
      "epoch 11: w= -0.011, loss=23.635\n",
      "epoch 21: w= 0.231, loss=16.960\n",
      "epoch 31: w= 0.436, loss=12.193\n",
      "epoch 41: w= 0.610, loss=8.789\n",
      "epoch 51: w= 0.757, loss=6.358\n",
      "epoch 61: w= 0.881, loss=4.621\n",
      "epoch 71: w= 0.986, loss=3.381\n",
      "epoch 81: w= 1.076, loss=2.495\n",
      "epoch 91: w= 1.151, loss=1.862\n",
      "epoch 101: w= 1.215, loss=1.409\n",
      "epoch 111: w= 1.269, loss=1.086\n",
      "epoch 121: w= 1.316, loss=0.854\n",
      "epoch 131: w= 1.355, loss=0.688\n",
      "epoch 141: w= 1.388, loss=0.569\n",
      "epoch 151: w= 1.416, loss=0.484\n",
      "epoch 161: w= 1.440, loss=0.423\n",
      "epoch 171: w= 1.461, loss=0.378\n",
      "epoch 181: w= 1.479, loss=0.346\n",
      "epoch 191: w= 1.494, loss=0.323\n",
      "epoch 201: w= 1.507, loss=0.306\n",
      "epoch 211: w= 1.518, loss=0.293\n",
      "epoch 221: w= 1.527, loss=0.284\n",
      "epoch 231: w= 1.536, loss=0.276\n",
      "epoch 241: w= 1.543, loss=0.271\n",
      "epoch 251: w= 1.549, loss=0.266\n",
      "epoch 261: w= 1.555, loss=0.263\n",
      "epoch 271: w= 1.559, loss=0.260\n",
      "epoch 281: w= 1.564, loss=0.257\n",
      "epoch 291: w= 1.568, loss=0.255\n",
      "epoch 301: w= 1.571, loss=0.253\n",
      "epoch 311: w= 1.574, loss=0.251\n",
      "epoch 321: w= 1.577, loss=0.249\n",
      "epoch 331: w= 1.579, loss=0.248\n",
      "epoch 341: w= 1.582, loss=0.246\n",
      "epoch 351: w= 1.584, loss=0.244\n",
      "epoch 361: w= 1.586, loss=0.243\n",
      "epoch 371: w= 1.588, loss=0.241\n",
      "epoch 381: w= 1.590, loss=0.240\n",
      "epoch 391: w= 1.591, loss=0.238\n",
      "epoch 401: w= 1.593, loss=0.237\n",
      "epoch 411: w= 1.594, loss=0.235\n",
      "epoch 421: w= 1.596, loss=0.234\n",
      "epoch 431: w= 1.597, loss=0.233\n",
      "epoch 441: w= 1.599, loss=0.231\n",
      "epoch 451: w= 1.600, loss=0.230\n",
      "epoch 461: w= 1.601, loss=0.229\n",
      "epoch 471: w= 1.603, loss=0.227\n",
      "epoch 481: w= 1.604, loss=0.226\n",
      "epoch 491: w= 1.605, loss=0.224\n",
      "epoch 501: w= 1.607, loss=0.223\n",
      "epoch 511: w= 1.608, loss=0.222\n",
      "epoch 521: w= 1.609, loss=0.220\n",
      "epoch 531: w= 1.610, loss=0.219\n",
      "epoch 541: w= 1.611, loss=0.218\n",
      "epoch 551: w= 1.613, loss=0.217\n",
      "epoch 561: w= 1.614, loss=0.215\n",
      "epoch 571: w= 1.615, loss=0.214\n",
      "epoch 581: w= 1.616, loss=0.213\n",
      "epoch 591: w= 1.617, loss=0.211\n",
      "epoch 601: w= 1.619, loss=0.210\n",
      "epoch 611: w= 1.620, loss=0.209\n",
      "epoch 621: w= 1.621, loss=0.208\n",
      "epoch 631: w= 1.622, loss=0.206\n",
      "epoch 641: w= 1.623, loss=0.205\n",
      "epoch 651: w= 1.624, loss=0.204\n",
      "epoch 661: w= 1.625, loss=0.203\n",
      "epoch 671: w= 1.626, loss=0.202\n",
      "epoch 681: w= 1.628, loss=0.200\n",
      "epoch 691: w= 1.629, loss=0.199\n",
      "epoch 701: w= 1.630, loss=0.198\n",
      "epoch 711: w= 1.631, loss=0.197\n",
      "epoch 721: w= 1.632, loss=0.196\n",
      "epoch 731: w= 1.633, loss=0.194\n",
      "epoch 741: w= 1.634, loss=0.193\n",
      "epoch 751: w= 1.635, loss=0.192\n",
      "epoch 761: w= 1.636, loss=0.191\n",
      "epoch 771: w= 1.638, loss=0.190\n",
      "epoch 781: w= 1.639, loss=0.189\n",
      "epoch 791: w= 1.640, loss=0.188\n",
      "epoch 801: w= 1.641, loss=0.186\n",
      "epoch 811: w= 1.642, loss=0.185\n",
      "epoch 821: w= 1.643, loss=0.184\n",
      "epoch 831: w= 1.644, loss=0.183\n",
      "epoch 841: w= 1.645, loss=0.182\n",
      "epoch 851: w= 1.646, loss=0.181\n",
      "epoch 861: w= 1.647, loss=0.180\n",
      "epoch 871: w= 1.648, loss=0.179\n",
      "epoch 881: w= 1.649, loss=0.178\n",
      "epoch 891: w= 1.650, loss=0.177\n",
      "epoch 901: w= 1.651, loss=0.176\n",
      "epoch 911: w= 1.652, loss=0.175\n",
      "epoch 921: w= 1.653, loss=0.173\n",
      "epoch 931: w= 1.654, loss=0.172\n",
      "epoch 941: w= 1.656, loss=0.171\n",
      "epoch 951: w= 1.657, loss=0.170\n",
      "epoch 961: w= 1.658, loss=0.169\n",
      "epoch 971: w= 1.659, loss=0.168\n",
      "epoch 981: w= 1.660, loss=0.167\n",
      "epoch 991: w= 1.661, loss=0.166\n",
      " prediction after training: f(5) = 9.303\n"
     ]
    }
   ],
   "source": [
    "# Construct the whole training pipelines\n",
    "# gradian decent with pythorch\n",
    "# 1) Design model (input, output, forward)\n",
    "# 2) Construct loss and optimize\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute the prediction\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Design models\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_sample, n_feature = X.shape\n",
    "\n",
    "input_size = n_feature\n",
    "output_size = n_feature\n",
    "\n",
    "# the first to create the linear model\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# create pytorch model\n",
    "# the second to create the linear model\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim,output_dim)\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "n_iters = 1000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f'Prediction before training: f(5): {model(X_test).item()}')\n",
    "for epoch in range(n_iters):\n",
    "    # forward\n",
    "    y_predict = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_predict)\n",
    "\n",
    "    # backward\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w= {w[0].item():.3f}, loss={l:.3f}')\n",
    "\n",
    "print(f' prediction after training: f(5) = {model(X_test).item():.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "epoch0, loss = 9803.3223\n",
      "epoch10, loss = 9347.5889\n",
      "epoch20, loss = 8913.9473\n",
      "epoch30, loss = 8501.3242\n",
      "epoch40, loss = 8108.7026\n",
      "epoch50, loss = 7735.1123\n",
      "epoch60, loss = 7379.6289\n",
      "epoch70, loss = 7041.3745\n",
      "epoch80, loss = 6719.5142\n",
      "epoch90, loss = 6413.2515\n",
      "epoch100, loss = 6121.8320\n",
      "epoch110, loss = 5844.5337\n",
      "epoch120, loss = 5580.6733\n",
      "epoch130, loss = 5329.5986\n",
      "epoch140, loss = 5090.6904\n",
      "epoch150, loss = 4863.3574\n",
      "epoch160, loss = 4647.0381\n",
      "epoch170, loss = 4441.2002\n",
      "epoch180, loss = 4245.3340\n",
      "epoch190, loss = 4058.9573\n",
      "epoch200, loss = 3881.6082\n",
      "epoch210, loss = 3712.8506\n",
      "epoch220, loss = 3552.2681\n",
      "epoch230, loss = 3399.4631\n",
      "epoch240, loss = 3254.0603\n",
      "epoch250, loss = 3115.6990\n",
      "epoch260, loss = 2984.0393\n",
      "epoch270, loss = 2858.7554\n",
      "epoch280, loss = 2739.5383\n",
      "epoch290, loss = 2626.0950\n",
      "epoch300, loss = 2518.1443\n",
      "epoch310, loss = 2415.4209\n",
      "epoch320, loss = 2317.6711\n",
      "epoch330, loss = 2224.6545\n",
      "epoch340, loss = 2136.1409\n",
      "epoch350, loss = 2051.9126\n",
      "epoch360, loss = 1971.7622\n",
      "epoch370, loss = 1895.4912\n",
      "epoch380, loss = 1822.9124\n",
      "epoch390, loss = 1753.8462\n",
      "epoch400, loss = 1688.1232\n",
      "epoch410, loss = 1625.5803\n",
      "epoch420, loss = 1566.0643\n",
      "epoch430, loss = 1509.4287\n",
      "epoch440, loss = 1455.5344\n",
      "epoch450, loss = 1404.2478\n",
      "epoch460, loss = 1355.4425\n",
      "epoch470, loss = 1308.9988\n",
      "epoch480, loss = 1264.8018\n",
      "epoch490, loss = 1222.7432\n",
      "epoch500, loss = 1182.7192\n",
      "epoch510, loss = 1144.6316\n",
      "epoch520, loss = 1108.3857\n",
      "epoch530, loss = 1073.8934\n",
      "epoch540, loss = 1041.0692\n",
      "epoch550, loss = 1009.8320\n",
      "epoch560, loss = 980.1050\n",
      "epoch570, loss = 951.8161\n",
      "epoch580, loss = 924.8949\n",
      "epoch590, loss = 899.2756\n",
      "epoch600, loss = 874.8957\n",
      "epoch610, loss = 851.6936\n",
      "epoch620, loss = 829.6135\n",
      "epoch630, loss = 808.5998\n",
      "epoch640, loss = 788.6028\n",
      "epoch650, loss = 769.5723\n",
      "epoch660, loss = 751.4617\n",
      "epoch670, loss = 734.2258\n",
      "epoch680, loss = 717.8228\n",
      "epoch690, loss = 702.2123\n",
      "epoch700, loss = 687.3566\n",
      "epoch710, loss = 673.2188\n",
      "epoch720, loss = 659.7643\n",
      "epoch730, loss = 646.9594\n",
      "epoch740, loss = 634.7730\n",
      "epoch750, loss = 623.1755\n",
      "epoch760, loss = 612.1379\n",
      "epoch770, loss = 601.6332\n",
      "epoch780, loss = 591.6361\n",
      "epoch790, loss = 582.1214\n",
      "epoch800, loss = 573.0665\n",
      "epoch810, loss = 564.4486\n",
      "epoch820, loss = 556.2469\n",
      "epoch830, loss = 548.4412\n",
      "epoch840, loss = 541.0120\n",
      "epoch850, loss = 533.9418\n",
      "epoch860, loss = 527.2127\n",
      "epoch870, loss = 520.8085\n",
      "epoch880, loss = 514.7131\n",
      "epoch890, loss = 508.9118\n",
      "epoch900, loss = 503.3903\n",
      "epoch910, loss = 498.1357\n",
      "epoch920, loss = 493.1343\n",
      "epoch930, loss = 488.3741\n",
      "epoch940, loss = 483.8436\n",
      "epoch950, loss = 479.5317\n",
      "epoch960, loss = 475.4275\n",
      "epoch970, loss = 471.5214\n",
      "epoch980, loss = 467.8036\n",
      "epoch990, loss = 464.2650\n",
      "epoch1000, loss = 460.8968\n",
      "epoch1010, loss = 457.6911\n",
      "epoch1020, loss = 454.6400\n",
      "epoch1030, loss = 451.7360\n",
      "epoch1040, loss = 448.9720\n",
      "epoch1050, loss = 446.3411\n",
      "epoch1060, loss = 443.8371\n",
      "epoch1070, loss = 441.4535\n",
      "epoch1080, loss = 439.1849\n",
      "epoch1090, loss = 437.0257\n",
      "epoch1100, loss = 434.9703\n",
      "epoch1110, loss = 433.0139\n",
      "epoch1120, loss = 431.1518\n",
      "epoch1130, loss = 429.3794\n",
      "epoch1140, loss = 427.6923\n",
      "epoch1150, loss = 426.0864\n",
      "epoch1160, loss = 424.5577\n",
      "epoch1170, loss = 423.1027\n",
      "epoch1180, loss = 421.7179\n",
      "epoch1190, loss = 420.3996\n",
      "epoch1200, loss = 419.1447\n",
      "epoch1210, loss = 417.9502\n",
      "epoch1220, loss = 416.8131\n",
      "epoch1230, loss = 415.7308\n",
      "epoch1240, loss = 414.7005\n",
      "epoch1250, loss = 413.7198\n",
      "epoch1260, loss = 412.7863\n",
      "epoch1270, loss = 411.8976\n",
      "epoch1280, loss = 411.0516\n",
      "epoch1290, loss = 410.2463\n",
      "epoch1300, loss = 409.4797\n",
      "epoch1310, loss = 408.7500\n",
      "epoch1320, loss = 408.0554\n",
      "epoch1330, loss = 407.3940\n",
      "epoch1340, loss = 406.7645\n",
      "epoch1350, loss = 406.1652\n",
      "epoch1360, loss = 405.5948\n",
      "epoch1370, loss = 405.0517\n",
      "epoch1380, loss = 404.5347\n",
      "epoch1390, loss = 404.0426\n",
      "epoch1400, loss = 403.5741\n",
      "epoch1410, loss = 403.1280\n",
      "epoch1420, loss = 402.7033\n",
      "epoch1430, loss = 402.2991\n",
      "epoch1440, loss = 401.9142\n",
      "epoch1450, loss = 401.5478\n",
      "epoch1460, loss = 401.1990\n",
      "epoch1470, loss = 400.8669\n",
      "epoch1480, loss = 400.5507\n",
      "epoch1490, loss = 400.2497\n",
      "epoch1500, loss = 399.9632\n",
      "epoch1510, loss = 399.6903\n",
      "epoch1520, loss = 399.4307\n",
      "epoch1530, loss = 399.1834\n",
      "epoch1540, loss = 398.9479\n",
      "epoch1550, loss = 398.7238\n",
      "epoch1560, loss = 398.5104\n",
      "epoch1570, loss = 398.3072\n",
      "epoch1580, loss = 398.1137\n",
      "epoch1590, loss = 397.9295\n",
      "epoch1600, loss = 397.7542\n",
      "epoch1610, loss = 397.5872\n",
      "epoch1620, loss = 397.4282\n",
      "epoch1630, loss = 397.2768\n",
      "epoch1640, loss = 397.1327\n",
      "epoch1650, loss = 396.9954\n",
      "epoch1660, loss = 396.8647\n",
      "epoch1670, loss = 396.7403\n",
      "epoch1680, loss = 396.6219\n",
      "epoch1690, loss = 396.5090\n",
      "epoch1700, loss = 396.4016\n",
      "epoch1710, loss = 396.2993\n",
      "epoch1720, loss = 396.2020\n",
      "epoch1730, loss = 396.1093\n",
      "epoch1740, loss = 396.0210\n",
      "epoch1750, loss = 395.9368\n",
      "epoch1760, loss = 395.8568\n",
      "epoch1770, loss = 395.7805\n",
      "epoch1780, loss = 395.7079\n",
      "epoch1790, loss = 395.6388\n",
      "epoch1800, loss = 395.5729\n",
      "epoch1810, loss = 395.5102\n",
      "epoch1820, loss = 395.4505\n",
      "epoch1830, loss = 395.3937\n",
      "epoch1840, loss = 395.3395\n",
      "epoch1850, loss = 395.2879\n",
      "epoch1860, loss = 395.2388\n",
      "epoch1870, loss = 395.1921\n",
      "epoch1880, loss = 395.1476\n",
      "epoch1890, loss = 395.1052\n",
      "epoch1900, loss = 395.0648\n",
      "epoch1910, loss = 395.0263\n",
      "epoch1920, loss = 394.9897\n",
      "epoch1930, loss = 394.9548\n",
      "epoch1940, loss = 394.9216\n",
      "epoch1950, loss = 394.8898\n",
      "epoch1960, loss = 394.8597\n",
      "epoch1970, loss = 394.8310\n",
      "epoch1980, loss = 394.8037\n",
      "epoch1990, loss = 394.7776\n",
      "epoch2000, loss = 394.7528\n",
      "epoch2010, loss = 394.7292\n",
      "epoch2020, loss = 394.7067\n",
      "epoch2030, loss = 394.6852\n",
      "epoch2040, loss = 394.6648\n",
      "epoch2050, loss = 394.6454\n",
      "epoch2060, loss = 394.6269\n",
      "epoch2070, loss = 394.6092\n",
      "epoch2080, loss = 394.5924\n",
      "epoch2090, loss = 394.5764\n",
      "epoch2100, loss = 394.5611\n",
      "epoch2110, loss = 394.5467\n",
      "epoch2120, loss = 394.5328\n",
      "epoch2130, loss = 394.5197\n",
      "epoch2140, loss = 394.5071\n",
      "epoch2150, loss = 394.4951\n",
      "epoch2160, loss = 394.4837\n",
      "epoch2170, loss = 394.4728\n",
      "epoch2180, loss = 394.4625\n",
      "epoch2190, loss = 394.4527\n",
      "epoch2200, loss = 394.4433\n",
      "epoch2210, loss = 394.4343\n",
      "epoch2220, loss = 394.4258\n",
      "epoch2230, loss = 394.4177\n",
      "epoch2240, loss = 394.4100\n",
      "epoch2250, loss = 394.4026\n",
      "epoch2260, loss = 394.3956\n",
      "epoch2270, loss = 394.3889\n",
      "epoch2280, loss = 394.3826\n",
      "epoch2290, loss = 394.3765\n",
      "epoch2300, loss = 394.3707\n",
      "epoch2310, loss = 394.3651\n",
      "epoch2320, loss = 394.3600\n",
      "epoch2330, loss = 394.3549\n",
      "epoch2340, loss = 394.3502\n",
      "epoch2350, loss = 394.3456\n",
      "epoch2360, loss = 394.3413\n",
      "epoch2370, loss = 394.3372\n",
      "epoch2380, loss = 394.3333\n",
      "epoch2390, loss = 394.3295\n",
      "epoch2400, loss = 394.3260\n",
      "epoch2410, loss = 394.3225\n",
      "epoch2420, loss = 394.3193\n",
      "epoch2430, loss = 394.3163\n",
      "epoch2440, loss = 394.3133\n",
      "epoch2450, loss = 394.3105\n",
      "epoch2460, loss = 394.3078\n",
      "epoch2470, loss = 394.3053\n",
      "epoch2480, loss = 394.3029\n",
      "epoch2490, loss = 394.3005\n",
      "epoch2500, loss = 394.2984\n",
      "epoch2510, loss = 394.2962\n",
      "epoch2520, loss = 394.2943\n",
      "epoch2530, loss = 394.2923\n",
      "epoch2540, loss = 394.2905\n",
      "epoch2550, loss = 394.2887\n",
      "epoch2560, loss = 394.2871\n",
      "epoch2570, loss = 394.2856\n",
      "epoch2580, loss = 394.2841\n",
      "epoch2590, loss = 394.2826\n",
      "epoch2600, loss = 394.2812\n",
      "epoch2610, loss = 394.2800\n",
      "epoch2620, loss = 394.2787\n",
      "epoch2630, loss = 394.2775\n",
      "epoch2640, loss = 394.2764\n",
      "epoch2650, loss = 394.2753\n",
      "epoch2660, loss = 394.2743\n",
      "epoch2670, loss = 394.2733\n",
      "epoch2680, loss = 394.2724\n",
      "epoch2690, loss = 394.2715\n",
      "epoch2700, loss = 394.2707\n",
      "epoch2710, loss = 394.2699\n",
      "epoch2720, loss = 394.2691\n",
      "epoch2730, loss = 394.2684\n",
      "epoch2740, loss = 394.2677\n",
      "epoch2750, loss = 394.2670\n",
      "epoch2760, loss = 394.2663\n",
      "epoch2770, loss = 394.2658\n",
      "epoch2780, loss = 394.2652\n",
      "epoch2790, loss = 394.2646\n",
      "epoch2800, loss = 394.2641\n",
      "epoch2810, loss = 394.2636\n",
      "epoch2820, loss = 394.2632\n",
      "epoch2830, loss = 394.2627\n",
      "epoch2840, loss = 394.2623\n",
      "epoch2850, loss = 394.2618\n",
      "epoch2860, loss = 394.2615\n",
      "epoch2870, loss = 394.2610\n",
      "epoch2880, loss = 394.2607\n",
      "epoch2890, loss = 394.2603\n",
      "epoch2900, loss = 394.2600\n",
      "epoch2910, loss = 394.2598\n",
      "epoch2920, loss = 394.2595\n",
      "epoch2930, loss = 394.2592\n",
      "epoch2940, loss = 394.2589\n",
      "epoch2950, loss = 394.2586\n",
      "epoch2960, loss = 394.2584\n",
      "epoch2970, loss = 394.2582\n",
      "epoch2980, loss = 394.2580\n",
      "epoch2990, loss = 394.2577\n",
      "epoch3000, loss = 394.2575\n",
      "epoch3010, loss = 394.2574\n",
      "epoch3020, loss = 394.2571\n",
      "epoch3030, loss = 394.2570\n",
      "epoch3040, loss = 394.2567\n",
      "epoch3050, loss = 394.2567\n",
      "epoch3060, loss = 394.2565\n",
      "epoch3070, loss = 394.2563\n",
      "epoch3080, loss = 394.2561\n",
      "epoch3090, loss = 394.2560\n",
      "epoch3100, loss = 394.2560\n",
      "epoch3110, loss = 394.2558\n",
      "epoch3120, loss = 394.2557\n",
      "epoch3130, loss = 394.2556\n",
      "epoch3140, loss = 394.2554\n",
      "epoch3150, loss = 394.2554\n",
      "epoch3160, loss = 394.2553\n",
      "epoch3170, loss = 394.2552\n",
      "epoch3180, loss = 394.2551\n",
      "epoch3190, loss = 394.2551\n",
      "epoch3200, loss = 394.2549\n",
      "epoch3210, loss = 394.2549\n",
      "epoch3220, loss = 394.2548\n",
      "epoch3230, loss = 394.2547\n",
      "epoch3240, loss = 394.2547\n",
      "epoch3250, loss = 394.2546\n",
      "epoch3260, loss = 394.2546\n",
      "epoch3270, loss = 394.2545\n",
      "epoch3280, loss = 394.2544\n",
      "epoch3290, loss = 394.2544\n",
      "epoch3300, loss = 394.2543\n",
      "epoch3310, loss = 394.2543\n",
      "epoch3320, loss = 394.2542\n",
      "epoch3330, loss = 394.2542\n",
      "epoch3340, loss = 394.2542\n",
      "epoch3350, loss = 394.2542\n",
      "epoch3360, loss = 394.2541\n",
      "epoch3370, loss = 394.2540\n",
      "epoch3380, loss = 394.2540\n",
      "epoch3390, loss = 394.2540\n",
      "epoch3400, loss = 394.2539\n",
      "epoch3410, loss = 394.2539\n",
      "epoch3420, loss = 394.2539\n",
      "epoch3430, loss = 394.2538\n",
      "epoch3440, loss = 394.2538\n",
      "epoch3450, loss = 394.2538\n",
      "epoch3460, loss = 394.2538\n",
      "epoch3470, loss = 394.2538\n",
      "epoch3480, loss = 394.2537\n",
      "epoch3490, loss = 394.2537\n",
      "epoch3500, loss = 394.2537\n",
      "epoch3510, loss = 394.2537\n",
      "epoch3520, loss = 394.2537\n",
      "epoch3530, loss = 394.2536\n",
      "epoch3540, loss = 394.2536\n",
      "epoch3550, loss = 394.2536\n",
      "epoch3560, loss = 394.2535\n",
      "epoch3570, loss = 394.2535\n",
      "epoch3580, loss = 394.2536\n",
      "epoch3590, loss = 394.2536\n",
      "epoch3600, loss = 394.2535\n",
      "epoch3610, loss = 394.2535\n",
      "epoch3620, loss = 394.2535\n",
      "epoch3630, loss = 394.2535\n",
      "epoch3640, loss = 394.2535\n",
      "epoch3650, loss = 394.2535\n",
      "epoch3660, loss = 394.2535\n",
      "epoch3670, loss = 394.2535\n",
      "epoch3680, loss = 394.2535\n",
      "epoch3690, loss = 394.2535\n",
      "epoch3700, loss = 394.2534\n",
      "epoch3710, loss = 394.2534\n",
      "epoch3720, loss = 394.2534\n",
      "epoch3730, loss = 394.2534\n",
      "epoch3740, loss = 394.2534\n",
      "epoch3750, loss = 394.2534\n",
      "epoch3760, loss = 394.2534\n",
      "epoch3770, loss = 394.2534\n",
      "epoch3780, loss = 394.2534\n",
      "epoch3790, loss = 394.2534\n",
      "epoch3800, loss = 394.2534\n",
      "epoch3810, loss = 394.2534\n",
      "epoch3820, loss = 394.2534\n",
      "epoch3830, loss = 394.2534\n",
      "epoch3840, loss = 394.2534\n",
      "epoch3850, loss = 394.2534\n",
      "epoch3860, loss = 394.2534\n",
      "epoch3870, loss = 394.2534\n",
      "epoch3880, loss = 394.2534\n",
      "epoch3890, loss = 394.2533\n",
      "epoch3900, loss = 394.2534\n",
      "epoch3910, loss = 394.2534\n",
      "epoch3920, loss = 394.2533\n",
      "epoch3930, loss = 394.2533\n",
      "epoch3940, loss = 394.2534\n",
      "epoch3950, loss = 394.2533\n",
      "epoch3960, loss = 394.2533\n",
      "epoch3970, loss = 394.2533\n",
      "epoch3980, loss = 394.2533\n",
      "epoch3990, loss = 394.2533\n",
      "epoch4000, loss = 394.2534\n",
      "epoch4010, loss = 394.2533\n",
      "epoch4020, loss = 394.2533\n",
      "epoch4030, loss = 394.2533\n",
      "epoch4040, loss = 394.2533\n",
      "epoch4050, loss = 394.2533\n",
      "epoch4060, loss = 394.2534\n",
      "epoch4070, loss = 394.2534\n",
      "epoch4080, loss = 394.2533\n",
      "epoch4090, loss = 394.2533\n",
      "epoch4100, loss = 394.2533\n",
      "epoch4110, loss = 394.2533\n",
      "epoch4120, loss = 394.2533\n",
      "epoch4130, loss = 394.2532\n",
      "epoch4140, loss = 394.2533\n",
      "epoch4150, loss = 394.2533\n",
      "epoch4160, loss = 394.2533\n",
      "epoch4170, loss = 394.2532\n",
      "epoch4180, loss = 394.2533\n",
      "epoch4190, loss = 394.2533\n",
      "epoch4200, loss = 394.2533\n",
      "epoch4210, loss = 394.2532\n",
      "epoch4220, loss = 394.2533\n",
      "epoch4230, loss = 394.2532\n",
      "epoch4240, loss = 394.2533\n",
      "epoch4250, loss = 394.2533\n",
      "epoch4260, loss = 394.2533\n",
      "epoch4270, loss = 394.2533\n",
      "epoch4280, loss = 394.2533\n",
      "epoch4290, loss = 394.2533\n",
      "epoch4300, loss = 394.2533\n",
      "epoch4310, loss = 394.2532\n",
      "epoch4320, loss = 394.2533\n",
      "epoch4330, loss = 394.2533\n",
      "epoch4340, loss = 394.2533\n",
      "epoch4350, loss = 394.2533\n",
      "epoch4360, loss = 394.2533\n",
      "epoch4370, loss = 394.2533\n",
      "epoch4380, loss = 394.2533\n",
      "epoch4390, loss = 394.2532\n",
      "epoch4400, loss = 394.2533\n",
      "epoch4410, loss = 394.2533\n",
      "epoch4420, loss = 394.2533\n",
      "epoch4430, loss = 394.2532\n",
      "epoch4440, loss = 394.2533\n",
      "epoch4450, loss = 394.2534\n",
      "epoch4460, loss = 394.2533\n",
      "epoch4470, loss = 394.2533\n",
      "epoch4480, loss = 394.2533\n",
      "epoch4490, loss = 394.2533\n",
      "epoch4500, loss = 394.2533\n",
      "epoch4510, loss = 394.2533\n",
      "epoch4520, loss = 394.2533\n",
      "epoch4530, loss = 394.2533\n",
      "epoch4540, loss = 394.2533\n",
      "epoch4550, loss = 394.2532\n",
      "epoch4560, loss = 394.2533\n",
      "epoch4570, loss = 394.2533\n",
      "epoch4580, loss = 394.2533\n",
      "epoch4590, loss = 394.2533\n",
      "epoch4600, loss = 394.2533\n",
      "epoch4610, loss = 394.2533\n",
      "epoch4620, loss = 394.2533\n",
      "epoch4630, loss = 394.2532\n",
      "epoch4640, loss = 394.2533\n",
      "epoch4650, loss = 394.2533\n",
      "epoch4660, loss = 394.2533\n",
      "epoch4670, loss = 394.2533\n",
      "epoch4680, loss = 394.2533\n",
      "epoch4690, loss = 394.2532\n",
      "epoch4700, loss = 394.2533\n",
      "epoch4710, loss = 394.2533\n",
      "epoch4720, loss = 394.2532\n",
      "epoch4730, loss = 394.2533\n",
      "epoch4740, loss = 394.2533\n",
      "epoch4750, loss = 394.2532\n",
      "epoch4760, loss = 394.2532\n",
      "epoch4770, loss = 394.2533\n",
      "epoch4780, loss = 394.2533\n",
      "epoch4790, loss = 394.2532\n",
      "epoch4800, loss = 394.2533\n",
      "epoch4810, loss = 394.2533\n",
      "epoch4820, loss = 394.2532\n",
      "epoch4830, loss = 394.2532\n",
      "epoch4840, loss = 394.2532\n",
      "epoch4850, loss = 394.2533\n",
      "epoch4860, loss = 394.2533\n",
      "epoch4870, loss = 394.2533\n",
      "epoch4880, loss = 394.2532\n",
      "epoch4890, loss = 394.2533\n",
      "epoch4900, loss = 394.2532\n",
      "epoch4910, loss = 394.2532\n",
      "epoch4920, loss = 394.2532\n",
      "epoch4930, loss = 394.2533\n",
      "epoch4940, loss = 394.2533\n",
      "epoch4950, loss = 394.2533\n",
      "epoch4960, loss = 394.2533\n",
      "epoch4970, loss = 394.2532\n",
      "epoch4980, loss = 394.2532\n",
      "epoch4990, loss = 394.2532\n",
      "epoch5000, loss = 394.2533\n",
      "epoch5010, loss = 394.2533\n",
      "epoch5020, loss = 394.2532\n",
      "epoch5030, loss = 394.2533\n",
      "epoch5040, loss = 394.2533\n",
      "epoch5050, loss = 394.2532\n",
      "epoch5060, loss = 394.2532\n",
      "epoch5070, loss = 394.2533\n",
      "epoch5080, loss = 394.2532\n",
      "epoch5090, loss = 394.2533\n",
      "epoch5100, loss = 394.2533\n",
      "epoch5110, loss = 394.2532\n",
      "epoch5120, loss = 394.2533\n",
      "epoch5130, loss = 394.2532\n",
      "epoch5140, loss = 394.2533\n",
      "epoch5150, loss = 394.2533\n",
      "epoch5160, loss = 394.2532\n",
      "epoch5170, loss = 394.2533\n",
      "epoch5180, loss = 394.2533\n",
      "epoch5190, loss = 394.2533\n",
      "epoch5200, loss = 394.2533\n",
      "epoch5210, loss = 394.2533\n",
      "epoch5220, loss = 394.2532\n",
      "epoch5230, loss = 394.2533\n",
      "epoch5240, loss = 394.2532\n",
      "epoch5250, loss = 394.2533\n",
      "epoch5260, loss = 394.2532\n",
      "epoch5270, loss = 394.2533\n",
      "epoch5280, loss = 394.2533\n",
      "epoch5290, loss = 394.2532\n",
      "epoch5300, loss = 394.2533\n",
      "epoch5310, loss = 394.2532\n",
      "epoch5320, loss = 394.2532\n",
      "epoch5330, loss = 394.2532\n",
      "epoch5340, loss = 394.2532\n",
      "epoch5350, loss = 394.2533\n",
      "epoch5360, loss = 394.2532\n",
      "epoch5370, loss = 394.2533\n",
      "epoch5380, loss = 394.2532\n",
      "epoch5390, loss = 394.2532\n",
      "epoch5400, loss = 394.2533\n",
      "epoch5410, loss = 394.2533\n",
      "epoch5420, loss = 394.2532\n",
      "epoch5430, loss = 394.2533\n",
      "epoch5440, loss = 394.2532\n",
      "epoch5450, loss = 394.2532\n",
      "epoch5460, loss = 394.2533\n",
      "epoch5470, loss = 394.2533\n",
      "epoch5480, loss = 394.2532\n",
      "epoch5490, loss = 394.2532\n",
      "epoch5500, loss = 394.2533\n",
      "epoch5510, loss = 394.2532\n",
      "epoch5520, loss = 394.2532\n",
      "epoch5530, loss = 394.2533\n",
      "epoch5540, loss = 394.2533\n",
      "epoch5550, loss = 394.2533\n",
      "epoch5560, loss = 394.2532\n",
      "epoch5570, loss = 394.2532\n",
      "epoch5580, loss = 394.2533\n",
      "epoch5590, loss = 394.2532\n",
      "epoch5600, loss = 394.2532\n",
      "epoch5610, loss = 394.2532\n",
      "epoch5620, loss = 394.2532\n",
      "epoch5630, loss = 394.2532\n",
      "epoch5640, loss = 394.2532\n",
      "epoch5650, loss = 394.2532\n",
      "epoch5660, loss = 394.2533\n",
      "epoch5670, loss = 394.2533\n",
      "epoch5680, loss = 394.2533\n",
      "epoch5690, loss = 394.2532\n",
      "epoch5700, loss = 394.2533\n",
      "epoch5710, loss = 394.2533\n",
      "epoch5720, loss = 394.2533\n",
      "epoch5730, loss = 394.2533\n",
      "epoch5740, loss = 394.2533\n",
      "epoch5750, loss = 394.2532\n",
      "epoch5760, loss = 394.2532\n",
      "epoch5770, loss = 394.2533\n",
      "epoch5780, loss = 394.2533\n",
      "epoch5790, loss = 394.2533\n",
      "epoch5800, loss = 394.2533\n",
      "epoch5810, loss = 394.2532\n",
      "epoch5820, loss = 394.2532\n",
      "epoch5830, loss = 394.2532\n",
      "epoch5840, loss = 394.2533\n",
      "epoch5850, loss = 394.2533\n",
      "epoch5860, loss = 394.2532\n",
      "epoch5870, loss = 394.2532\n",
      "epoch5880, loss = 394.2533\n",
      "epoch5890, loss = 394.2532\n",
      "epoch5900, loss = 394.2532\n",
      "epoch5910, loss = 394.2533\n",
      "epoch5920, loss = 394.2533\n",
      "epoch5930, loss = 394.2533\n",
      "epoch5940, loss = 394.2533\n",
      "epoch5950, loss = 394.2533\n",
      "epoch5960, loss = 394.2533\n",
      "epoch5970, loss = 394.2533\n",
      "epoch5980, loss = 394.2533\n",
      "epoch5990, loss = 394.2532\n",
      "epoch6000, loss = 394.2533\n",
      "epoch6010, loss = 394.2533\n",
      "epoch6020, loss = 394.2533\n",
      "epoch6030, loss = 394.2533\n",
      "epoch6040, loss = 394.2533\n",
      "epoch6050, loss = 394.2533\n",
      "epoch6060, loss = 394.2532\n",
      "epoch6070, loss = 394.2532\n",
      "epoch6080, loss = 394.2532\n",
      "epoch6090, loss = 394.2532\n",
      "epoch6100, loss = 394.2533\n",
      "epoch6110, loss = 394.2533\n",
      "epoch6120, loss = 394.2533\n",
      "epoch6130, loss = 394.2532\n",
      "epoch6140, loss = 394.2533\n",
      "epoch6150, loss = 394.2533\n",
      "epoch6160, loss = 394.2533\n",
      "epoch6170, loss = 394.2533\n",
      "epoch6180, loss = 394.2533\n",
      "epoch6190, loss = 394.2533\n",
      "epoch6200, loss = 394.2532\n",
      "epoch6210, loss = 394.2532\n",
      "epoch6220, loss = 394.2532\n",
      "epoch6230, loss = 394.2532\n",
      "epoch6240, loss = 394.2533\n",
      "epoch6250, loss = 394.2533\n",
      "epoch6260, loss = 394.2533\n",
      "epoch6270, loss = 394.2533\n",
      "epoch6280, loss = 394.2532\n",
      "epoch6290, loss = 394.2532\n",
      "epoch6300, loss = 394.2532\n",
      "epoch6310, loss = 394.2532\n",
      "epoch6320, loss = 394.2532\n",
      "epoch6330, loss = 394.2532\n",
      "epoch6340, loss = 394.2532\n",
      "epoch6350, loss = 394.2532\n",
      "epoch6360, loss = 394.2532\n",
      "epoch6370, loss = 394.2532\n",
      "epoch6380, loss = 394.2532\n",
      "epoch6390, loss = 394.2532\n",
      "epoch6400, loss = 394.2532\n",
      "epoch6410, loss = 394.2532\n",
      "epoch6420, loss = 394.2532\n",
      "epoch6430, loss = 394.2533\n",
      "epoch6440, loss = 394.2533\n",
      "epoch6450, loss = 394.2533\n",
      "epoch6460, loss = 394.2533\n",
      "epoch6470, loss = 394.2533\n",
      "epoch6480, loss = 394.2533\n",
      "epoch6490, loss = 394.2533\n",
      "epoch6500, loss = 394.2533\n",
      "epoch6510, loss = 394.2533\n",
      "epoch6520, loss = 394.2533\n",
      "epoch6530, loss = 394.2533\n",
      "epoch6540, loss = 394.2533\n",
      "epoch6550, loss = 394.2533\n",
      "epoch6560, loss = 394.2533\n",
      "epoch6570, loss = 394.2533\n",
      "epoch6580, loss = 394.2533\n",
      "epoch6590, loss = 394.2533\n",
      "epoch6600, loss = 394.2533\n",
      "epoch6610, loss = 394.2533\n",
      "epoch6620, loss = 394.2533\n",
      "epoch6630, loss = 394.2533\n",
      "epoch6640, loss = 394.2533\n",
      "epoch6650, loss = 394.2533\n",
      "epoch6660, loss = 394.2533\n",
      "epoch6670, loss = 394.2533\n",
      "epoch6680, loss = 394.2533\n",
      "epoch6690, loss = 394.2532\n",
      "epoch6700, loss = 394.2532\n",
      "epoch6710, loss = 394.2532\n",
      "epoch6720, loss = 394.2532\n",
      "epoch6730, loss = 394.2532\n",
      "epoch6740, loss = 394.2533\n",
      "epoch6750, loss = 394.2533\n",
      "epoch6760, loss = 394.2533\n",
      "epoch6770, loss = 394.2533\n",
      "epoch6780, loss = 394.2533\n",
      "epoch6790, loss = 394.2533\n",
      "epoch6800, loss = 394.2533\n",
      "epoch6810, loss = 394.2533\n",
      "epoch6820, loss = 394.2533\n",
      "epoch6830, loss = 394.2533\n",
      "epoch6840, loss = 394.2533\n",
      "epoch6850, loss = 394.2533\n",
      "epoch6860, loss = 394.2533\n",
      "epoch6870, loss = 394.2533\n",
      "epoch6880, loss = 394.2533\n",
      "epoch6890, loss = 394.2533\n",
      "epoch6900, loss = 394.2533\n",
      "epoch6910, loss = 394.2533\n",
      "epoch6920, loss = 394.2533\n",
      "epoch6930, loss = 394.2533\n",
      "epoch6940, loss = 394.2533\n",
      "epoch6950, loss = 394.2533\n",
      "epoch6960, loss = 394.2533\n",
      "epoch6970, loss = 394.2533\n",
      "epoch6980, loss = 394.2533\n",
      "epoch6990, loss = 394.2533\n",
      "epoch7000, loss = 394.2533\n",
      "epoch7010, loss = 394.2533\n",
      "epoch7020, loss = 394.2533\n",
      "epoch7030, loss = 394.2533\n",
      "epoch7040, loss = 394.2533\n",
      "epoch7050, loss = 394.2533\n",
      "epoch7060, loss = 394.2533\n",
      "epoch7070, loss = 394.2533\n",
      "epoch7080, loss = 394.2533\n",
      "epoch7090, loss = 394.2533\n",
      "epoch7100, loss = 394.2533\n",
      "epoch7110, loss = 394.2533\n",
      "epoch7120, loss = 394.2533\n",
      "epoch7130, loss = 394.2533\n",
      "epoch7140, loss = 394.2533\n",
      "epoch7150, loss = 394.2533\n",
      "epoch7160, loss = 394.2533\n",
      "epoch7170, loss = 394.2533\n",
      "epoch7180, loss = 394.2533\n",
      "epoch7190, loss = 394.2533\n",
      "epoch7200, loss = 394.2533\n",
      "epoch7210, loss = 394.2533\n",
      "epoch7220, loss = 394.2532\n",
      "epoch7230, loss = 394.2532\n",
      "epoch7240, loss = 394.2532\n",
      "epoch7250, loss = 394.2533\n",
      "epoch7260, loss = 394.2533\n",
      "epoch7270, loss = 394.2533\n",
      "epoch7280, loss = 394.2532\n",
      "epoch7290, loss = 394.2532\n",
      "epoch7300, loss = 394.2532\n",
      "epoch7310, loss = 394.2533\n",
      "epoch7320, loss = 394.2533\n",
      "epoch7330, loss = 394.2533\n",
      "epoch7340, loss = 394.2533\n",
      "epoch7350, loss = 394.2533\n",
      "epoch7360, loss = 394.2533\n",
      "epoch7370, loss = 394.2533\n",
      "epoch7380, loss = 394.2533\n",
      "epoch7390, loss = 394.2533\n",
      "epoch7400, loss = 394.2533\n",
      "epoch7410, loss = 394.2533\n",
      "epoch7420, loss = 394.2533\n",
      "epoch7430, loss = 394.2533\n",
      "epoch7440, loss = 394.2533\n",
      "epoch7450, loss = 394.2533\n",
      "epoch7460, loss = 394.2533\n",
      "epoch7470, loss = 394.2533\n",
      "epoch7480, loss = 394.2533\n",
      "epoch7490, loss = 394.2533\n",
      "epoch7500, loss = 394.2533\n",
      "epoch7510, loss = 394.2532\n",
      "epoch7520, loss = 394.2532\n",
      "epoch7530, loss = 394.2532\n",
      "epoch7540, loss = 394.2532\n",
      "epoch7550, loss = 394.2532\n",
      "epoch7560, loss = 394.2532\n",
      "epoch7570, loss = 394.2533\n",
      "epoch7580, loss = 394.2533\n",
      "epoch7590, loss = 394.2533\n",
      "epoch7600, loss = 394.2533\n",
      "epoch7610, loss = 394.2533\n",
      "epoch7620, loss = 394.2533\n",
      "epoch7630, loss = 394.2533\n",
      "epoch7640, loss = 394.2533\n",
      "epoch7650, loss = 394.2533\n",
      "epoch7660, loss = 394.2533\n",
      "epoch7670, loss = 394.2533\n",
      "epoch7680, loss = 394.2533\n",
      "epoch7690, loss = 394.2533\n",
      "epoch7700, loss = 394.2533\n",
      "epoch7710, loss = 394.2533\n",
      "epoch7720, loss = 394.2533\n",
      "epoch7730, loss = 394.2533\n",
      "epoch7740, loss = 394.2533\n",
      "epoch7750, loss = 394.2533\n",
      "epoch7760, loss = 394.2533\n",
      "epoch7770, loss = 394.2533\n",
      "epoch7780, loss = 394.2533\n",
      "epoch7790, loss = 394.2533\n",
      "epoch7800, loss = 394.2533\n",
      "epoch7810, loss = 394.2533\n",
      "epoch7820, loss = 394.2533\n",
      "epoch7830, loss = 394.2533\n",
      "epoch7840, loss = 394.2533\n",
      "epoch7850, loss = 394.2533\n",
      "epoch7860, loss = 394.2533\n",
      "epoch7870, loss = 394.2533\n",
      "epoch7880, loss = 394.2533\n",
      "epoch7890, loss = 394.2533\n",
      "epoch7900, loss = 394.2533\n",
      "epoch7910, loss = 394.2533\n",
      "epoch7920, loss = 394.2533\n",
      "epoch7930, loss = 394.2533\n",
      "epoch7940, loss = 394.2533\n",
      "epoch7950, loss = 394.2533\n",
      "epoch7960, loss = 394.2533\n",
      "epoch7970, loss = 394.2533\n",
      "epoch7980, loss = 394.2533\n",
      "epoch7990, loss = 394.2533\n",
      "epoch8000, loss = 394.2533\n",
      "epoch8010, loss = 394.2533\n",
      "epoch8020, loss = 394.2533\n",
      "epoch8030, loss = 394.2533\n",
      "epoch8040, loss = 394.2533\n",
      "epoch8050, loss = 394.2533\n",
      "epoch8060, loss = 394.2533\n",
      "epoch8070, loss = 394.2533\n",
      "epoch8080, loss = 394.2533\n",
      "epoch8090, loss = 394.2533\n",
      "epoch8100, loss = 394.2533\n",
      "epoch8110, loss = 394.2533\n",
      "epoch8120, loss = 394.2533\n",
      "epoch8130, loss = 394.2533\n",
      "epoch8140, loss = 394.2533\n",
      "epoch8150, loss = 394.2533\n",
      "epoch8160, loss = 394.2533\n",
      "epoch8170, loss = 394.2533\n",
      "epoch8180, loss = 394.2533\n",
      "epoch8190, loss = 394.2533\n",
      "epoch8200, loss = 394.2533\n",
      "epoch8210, loss = 394.2533\n",
      "epoch8220, loss = 394.2533\n",
      "epoch8230, loss = 394.2533\n",
      "epoch8240, loss = 394.2533\n",
      "epoch8250, loss = 394.2533\n",
      "epoch8260, loss = 394.2533\n",
      "epoch8270, loss = 394.2533\n",
      "epoch8280, loss = 394.2533\n",
      "epoch8290, loss = 394.2533\n",
      "epoch8300, loss = 394.2533\n",
      "epoch8310, loss = 394.2533\n",
      "epoch8320, loss = 394.2533\n",
      "epoch8330, loss = 394.2533\n",
      "epoch8340, loss = 394.2533\n",
      "epoch8350, loss = 394.2533\n",
      "epoch8360, loss = 394.2533\n",
      "epoch8370, loss = 394.2533\n",
      "epoch8380, loss = 394.2533\n",
      "epoch8390, loss = 394.2533\n",
      "epoch8400, loss = 394.2533\n",
      "epoch8410, loss = 394.2533\n",
      "epoch8420, loss = 394.2533\n",
      "epoch8430, loss = 394.2533\n",
      "epoch8440, loss = 394.2533\n",
      "epoch8450, loss = 394.2533\n",
      "epoch8460, loss = 394.2533\n",
      "epoch8470, loss = 394.2533\n",
      "epoch8480, loss = 394.2533\n",
      "epoch8490, loss = 394.2533\n",
      "epoch8500, loss = 394.2533\n",
      "epoch8510, loss = 394.2533\n",
      "epoch8520, loss = 394.2533\n",
      "epoch8530, loss = 394.2533\n",
      "epoch8540, loss = 394.2533\n",
      "epoch8550, loss = 394.2533\n",
      "epoch8560, loss = 394.2533\n",
      "epoch8570, loss = 394.2533\n",
      "epoch8580, loss = 394.2533\n",
      "epoch8590, loss = 394.2533\n",
      "epoch8600, loss = 394.2533\n",
      "epoch8610, loss = 394.2533\n",
      "epoch8620, loss = 394.2533\n",
      "epoch8630, loss = 394.2533\n",
      "epoch8640, loss = 394.2533\n",
      "epoch8650, loss = 394.2533\n",
      "epoch8660, loss = 394.2533\n",
      "epoch8670, loss = 394.2533\n",
      "epoch8680, loss = 394.2533\n",
      "epoch8690, loss = 394.2533\n",
      "epoch8700, loss = 394.2533\n",
      "epoch8710, loss = 394.2533\n",
      "epoch8720, loss = 394.2533\n",
      "epoch8730, loss = 394.2533\n",
      "epoch8740, loss = 394.2533\n",
      "epoch8750, loss = 394.2533\n",
      "epoch8760, loss = 394.2533\n",
      "epoch8770, loss = 394.2533\n",
      "epoch8780, loss = 394.2533\n",
      "epoch8790, loss = 394.2533\n",
      "epoch8800, loss = 394.2533\n",
      "epoch8810, loss = 394.2533\n",
      "epoch8820, loss = 394.2533\n",
      "epoch8830, loss = 394.2533\n",
      "epoch8840, loss = 394.2533\n",
      "epoch8850, loss = 394.2533\n",
      "epoch8860, loss = 394.2533\n",
      "epoch8870, loss = 394.2533\n",
      "epoch8880, loss = 394.2533\n",
      "epoch8890, loss = 394.2533\n",
      "epoch8900, loss = 394.2533\n",
      "epoch8910, loss = 394.2533\n",
      "epoch8920, loss = 394.2533\n",
      "epoch8930, loss = 394.2533\n",
      "epoch8940, loss = 394.2533\n",
      "epoch8950, loss = 394.2533\n",
      "epoch8960, loss = 394.2533\n",
      "epoch8970, loss = 394.2533\n",
      "epoch8980, loss = 394.2533\n",
      "epoch8990, loss = 394.2533\n",
      "epoch9000, loss = 394.2533\n",
      "epoch9010, loss = 394.2533\n",
      "epoch9020, loss = 394.2533\n",
      "epoch9030, loss = 394.2533\n",
      "epoch9040, loss = 394.2533\n",
      "epoch9050, loss = 394.2533\n",
      "epoch9060, loss = 394.2533\n",
      "epoch9070, loss = 394.2533\n",
      "epoch9080, loss = 394.2533\n",
      "epoch9090, loss = 394.2533\n",
      "epoch9100, loss = 394.2533\n",
      "epoch9110, loss = 394.2533\n",
      "epoch9120, loss = 394.2533\n",
      "epoch9130, loss = 394.2533\n",
      "epoch9140, loss = 394.2533\n",
      "epoch9150, loss = 394.2533\n",
      "epoch9160, loss = 394.2533\n",
      "epoch9170, loss = 394.2533\n",
      "epoch9180, loss = 394.2533\n",
      "epoch9190, loss = 394.2533\n",
      "epoch9200, loss = 394.2533\n",
      "epoch9210, loss = 394.2533\n",
      "epoch9220, loss = 394.2533\n",
      "epoch9230, loss = 394.2533\n",
      "epoch9240, loss = 394.2533\n",
      "epoch9250, loss = 394.2533\n",
      "epoch9260, loss = 394.2533\n",
      "epoch9270, loss = 394.2533\n",
      "epoch9280, loss = 394.2533\n",
      "epoch9290, loss = 394.2533\n",
      "epoch9300, loss = 394.2533\n",
      "epoch9310, loss = 394.2533\n",
      "epoch9320, loss = 394.2533\n",
      "epoch9330, loss = 394.2533\n",
      "epoch9340, loss = 394.2533\n",
      "epoch9350, loss = 394.2533\n",
      "epoch9360, loss = 394.2533\n",
      "epoch9370, loss = 394.2533\n",
      "epoch9380, loss = 394.2533\n",
      "epoch9390, loss = 394.2533\n",
      "epoch9400, loss = 394.2533\n",
      "epoch9410, loss = 394.2533\n",
      "epoch9420, loss = 394.2533\n",
      "epoch9430, loss = 394.2533\n",
      "epoch9440, loss = 394.2533\n",
      "epoch9450, loss = 394.2533\n",
      "epoch9460, loss = 394.2533\n",
      "epoch9470, loss = 394.2533\n",
      "epoch9480, loss = 394.2533\n",
      "epoch9490, loss = 394.2533\n",
      "epoch9500, loss = 394.2533\n",
      "epoch9510, loss = 394.2533\n",
      "epoch9520, loss = 394.2533\n",
      "epoch9530, loss = 394.2533\n",
      "epoch9540, loss = 394.2533\n",
      "epoch9550, loss = 394.2533\n",
      "epoch9560, loss = 394.2533\n",
      "epoch9570, loss = 394.2533\n",
      "epoch9580, loss = 394.2533\n",
      "epoch9590, loss = 394.2533\n",
      "epoch9600, loss = 394.2533\n",
      "epoch9610, loss = 394.2533\n",
      "epoch9620, loss = 394.2533\n",
      "epoch9630, loss = 394.2533\n",
      "epoch9640, loss = 394.2533\n",
      "epoch9650, loss = 394.2533\n",
      "epoch9660, loss = 394.2533\n",
      "epoch9670, loss = 394.2533\n",
      "epoch9680, loss = 394.2533\n",
      "epoch9690, loss = 394.2533\n",
      "epoch9700, loss = 394.2533\n",
      "epoch9710, loss = 394.2533\n",
      "epoch9720, loss = 394.2533\n",
      "epoch9730, loss = 394.2533\n",
      "epoch9740, loss = 394.2533\n",
      "epoch9750, loss = 394.2533\n",
      "epoch9760, loss = 394.2533\n",
      "epoch9770, loss = 394.2533\n",
      "epoch9780, loss = 394.2533\n",
      "epoch9790, loss = 394.2533\n",
      "epoch9800, loss = 394.2533\n",
      "epoch9810, loss = 394.2533\n",
      "epoch9820, loss = 394.2533\n",
      "epoch9830, loss = 394.2533\n",
      "epoch9840, loss = 394.2533\n",
      "epoch9850, loss = 394.2533\n",
      "epoch9860, loss = 394.2533\n",
      "epoch9870, loss = 394.2533\n",
      "epoch9880, loss = 394.2533\n",
      "epoch9890, loss = 394.2533\n",
      "epoch9900, loss = 394.2533\n",
      "epoch9910, loss = 394.2533\n",
      "epoch9920, loss = 394.2533\n",
      "epoch9930, loss = 394.2533\n",
      "epoch9940, loss = 394.2533\n",
      "epoch9950, loss = 394.2533\n",
      "epoch9960, loss = 394.2533\n",
      "epoch9970, loss = 394.2533\n",
      "epoch9980, loss = 394.2533\n",
      "epoch9990, loss = 394.2533\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct linear regression\n",
    "# gradian decent with pythorch\n",
    "# 1) Design model (input, output, forward)\n",
    "# 2) Construct loss and optimize\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute the prediction\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# prepare the data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100,n_features=1, noise=20, random_state = 69)\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0],1)\n",
    "\n",
    "n_sample, n_feature = X.shape\n",
    "print(X.shape)\n",
    "\n",
    "# design the model\n",
    "input_size = n_feature\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# define the loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# training loop\n",
    "n_iters = 10000\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_predict = model(X)\n",
    "    # get loss\n",
    "    loss = criterion(y_predict,y)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 10 ==0:\n",
    "        print(f'epoch{epoch}, loss = {loss.item():.4f}')\n",
    "\n",
    "#plot\n",
    "predict = model(X).detach().numpy() # detach to define gradient to False\n",
    "plt.plot(X_numpy, y_numpy, \"ro\");\n",
    "plt.plot(X_numpy, predict, \"g\");\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}